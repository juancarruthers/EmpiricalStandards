# Confiabilidad y acuerdo entre evaluadores <sup>[1](#sdfootnote1anc)</sup>
&quot;La medida en que diferentes evaluadores asignan el mismo valor preciso para cada elemento que se califica&quot; (Acuerdo entre evaluadores; IRA) y &quot;la medida en que los evaluadores pueden distinguir consistentemente entre diferentes elementos en una escala de medición&quot; (Confiabilidad entre evaluadores; IRR)<sup>[2](#sdfootnote2anc)</sup>

## Aplicación

Se aplica a estudios en los que uno o más evaluadores humanos (también llamados jueces o codificadores) califican (o miden, etiquetan, juzgan, clasifican o categorizan) una o más propiedades de uno o más objetos de investigación.<sup>[3](#sdfootnote3anc)</sup>

## ¿Se necesitan múltiples evaluadores?

No existe una regla universal para determinar cuándo son necesarios dos o más evaluadores. Deben tenerse en cuenta los siguientes factores:

- **Controversialidad:** cuanto más controvertido sea el juicio, se necesitan más evaluadores múltiples; por ejemplo, registrar el año de publicación de los estudios primarios en una Revision Sistemática de la Literatura es menos controvertido que evaluar la elegancia de una solución técnica.
- **Practicidad:** cuanto menos práctico es tener múltiples calificaciones, más razonable se vuelve el diseño de un solo evaluador; por ejemplo, es más práctico que múltiples evaluadores apliquen un esquema de codificación deductivo _a priori_ a algunos artefactos, a que múltiples evaluadores codifiquen inductivamente 2000 páginas de transcripciones de entrevistas.
- **Filosofía:** tener múltiples evaluadores es más importante desde una perspectiva ontológica realista (característica del positivismo y falsacionismo) que desde una perspectiva ontológica idealista (característica del interpretativismo y constructivismo).

## Atributos Específicos

### Atributos Esenciales
- [ ] Indica claramente qué propiedades fueron calificadas
- [ ] Indica claramente cuántos evaluadores calificaron cada propiedad

O BIEN:
  - [ ] Proporciona una justificación razonable para usar un solo evaluador<sup>[4](#sdfootnote4anc)</sup>

O:
  - [ ] Describe el proceso mediante el cual dos o más evaluadores calificaron independientemente las propiedades de los objetos de investigación; Y
  - [ ] Describe cómo se resolvieron los desacuerdos; E
  - [ ] Indica el tipo de variable (nominal, ordinal, intervalo, razón) de las calificaciones; E
  - [ ] Informa una medida estadística apropiada de IRR/IRA<sup>[5](#sdfootnote5anc)</sup> 

### Atributos Deseables
- [ ] Proporciona materiales complementarios que incluyen: esquema(s) de calificación, reglas de decisión, desacuerdos de ejemplo, IIR/IRA desglosados por propiedad u oleada de análisis.
- [ ] Justifica el estadístico utilizado<sup>[6](#sdfootnote6anc)</sup>
- [ ] Informa interpretaciones o umbrales establecidos para los estadísticos utilizados
- [ ] Analiza resultados anómalos<sup>[7](#sdfootnote7anc)</sup> a la luz de las propiedades del estadístico utilizado (por ejemplo, anomalías Kappa de Cohen<sup>[8](#sdfootnote8anc)</sup>)
- [ ] Describe la formación o experiencia de los evaluadores en el tema de investigación
- [ ] Resuelve los desacuerdos a través de la discusión (en lugar de votando)

### Atributos Extraordinarios
- [ ] Emplea a más de tres evaluadores por propiedad
- [ ] Informa un proceso iterativo con múltiples ciclos de (i) calificar un subconjunto de los datos, (ii) resolver desacuerdos, y (iii) actualizar el esquema de calificación o las reglas de decisión hasta que un umbral mínimo indique una confiabilidad/acuerdo aceptable; informa la IIR/IRA para cada ciclo en un proceso iterativo
- [ ] Calcula la IRR/IRA para la calidad interna de la investigación, es decir, como una herramienta para mejorar progresivamente la consistencia de los sistemas de calificación mejorando así la reflexividad de los investigadores

## Antipatrones

- Informar la IRR cuando la IRA sea más apropiada y viceversa.
- Pretender que el estadístico de IRR o IRA indica buena confiabilidad cuando está por debajo de los umbrales establecidos.
- Calcular múltiples medidas de IRR/IRA y reportar solo las más favorables (p-hacking)
- Pretender que un estudio obviamente positivista adopte una ontología idealista para evitar emplear múltiples evaluadores.

## Ejemplos de Desviaciones Aceptables

- Resolver desacuerdos usando un desempate en lugar de una discusión porque un evaluador no estaba disponible.
- Los materiales complementarios no incluyen ejemplos de desacuerdos porque los datos son confidenciales.
- No hay indicación del número de iteraciones realizadas para evaluar/mejorar la IRR/IRA.

## Críticas Inválidas

- Criticar el uso de un solo evaluador cuando múltiples evaluadores no serían prácticos o serían incompatibles con la filosofía subyacente del estudio.
- Criticar el uso de un solo evaluador cuando los datos son tales que no hay razón para sospechar que diferentes evaluadores llegarían a conclusiones diferentes.
- Criticar el uso de múltiples evaluadores. Es difícil imaginar un escenario en el que múltiples evaluadores perjudiquen activamente la credibilidad de un estudio.
- &#39;IRR/IRA demasiado bajo&#39; cuando no existe un umbral basado en la evidencia y las amenazas a la confiabilidad se reconocen claramente.

## Notas

- La IRR es una medida de correlación que se puede calcular utilizando (por ejemplo) r de Pearson, tau de Kendall o rho de Spearman
- La IRA es una medida de acuerdo y se puede calcular usando (por ejemplo) pi de Scott, kappa de Cohen, kappa de Fleiss y alfa de Krippendorff.
- El análisis IRR/IRA no solo indica confiabilidad y objetividad (en la investigación positivista) sino que también mejora la reflexividad (en la investigación anti-positivista).

## Lecturas Sugeridas
FENG, G. C. 2014. Intercoder reliability indices: disuse, misuse, and abuse. _Quality &amp; Quantity_, 48 (3), 1803-1815.  

GISEV, N., BELL, J. S., &amp; CHEN, T. F. 2013. Interrater agreement and interrater reliability: key concepts, approaches, and applications. _Research in Social and Administrative Pharmacy_, 9 (3), 330-338.  

HENRICA C.W. DE VETA, CAROLINE B. TERWEEA, DIRK L. KNOLA,B, LEX M. BOUTER. 2006. When to use agreement versus reliability measures. _Journal of Clinical Epidemiology_ 59, 1033–1039.  

NILI, A., TATE, M., BARROS, A., &amp; JOHNSTONE, D. 2020. An approach for selecting and using a method of inter-coder reliability in information management research. _International Journal of Information Management_, 54.  

O&#39;CONNOR, C., &amp; JOFFE, H. 2020. Intercoder reliability in qualitative research: debates and practical guidelines. _International Journal of Qualitative Methods_, 19.  

## Ejemplares
JESSICA DIAZ, DANIEL LÓPEZ-FERNÁNDEZ, JORGE PEREZ, ÁNGEL GONZÁLEZ-PRIETO (in press) Why are many businesses instilling a DevOps culture into their organization? _Empirical Software Engineering  

JORGE PÉREZ, JESSICA DÍAZ, JAVIER GARCÍA-MARTÍN, AND BERNARDO TABUENCA. 2020. Systematic literature reviews in software engineering - enhancement of the study selection process using Cohen&#39;s Kappa statistic. _Journal of Systems and Software_, 168  

JORGE PÉREZ, CARMEN VIZCARRO, JAVIER GARCÍA, AURELIO BERMÚDEZ, AND RUTH COBOS.2017. Development of Procedures to Assess Problem-Solving Competence in Computing Engineering. _IEEE Transactions on Education_, 60 (1), 22-28  

R. MOHANANI, B. TURHAN, P. RALPH, (in press) Requirements framing affects design creativity. _IEEE Transactions on Software Engineering._ DOI: 10.1109/TSE.2019.2909033  

ZAPF, A., CASTELL, S., MORAWIETZ, L., &amp; KARCH, A. 2016. Measuring inter-rater reliability for nominal data–which coefficients and confidence intervals are appropriate? _BMC medical research methodology_, 16, article 93  

---
[1](#sdfootnote1anc) Evaluar la consistencia entre los evaluadores, cuando corresponda, promueve la &quot;_sistematicidad, la comunicabilidad y la transparencia del proceso de codificación; reflexividad y diálogo dentro de los equipos de investigación; y ayuda a satisfacer a diversas audiencias de la confiabilidad de la investigación_&quot; (O&#39;Connor &amp; Joffe 2020).  
[2](#sdfootnote2anc) Consultar Gisev, Bell, &amp; Chen (2013)  
[3](#sdfootnote3anc) Por ejemplo: (a) aplicar criterios de selección en una Revisión Sistemática de la Literatura; (b) un experimento en el que un ser humano puntúa una variable dependiente como la calidad del código; (c) análisis cualitativo deductivo con un esquema de codificación a priori en un estudio de caso positivista.  
[4](#sdfootnote4anc) Por Ejemplo, las calificaciones no son controvertidas; múltiples evaluadores no serían prácticos; múltiples evaluadores son inconsistentes con la perspectiva filosófica del trabajo  
[5](#sdfootnote5anc) Tales como R de Pearson, tau de Kendall, rho de Spearman, Kappa de Cohen, Kappa de Fleiss, Alpha de Krippendorff o (inusualmente) porcentaje de acuerdo 
[6](#sdfootnote6anc) Por ejemplo, ¿Es importante el valor absoluto de cada elemento que se califica (IRA) o la tendencia en las calificaciones (IRR)?  
[7](#sdfootnote7anc) Por ejemplo, valor alto de acuerdo observado, pero valor bajo del estadístico o viceversa  
[8](#sdfootnote8anc) Consultar FEINSTEIN, A. R., &amp; CICCHETTI, D. V.1990. High Agreement But Low Kappa: I. the Problems of Two Paradoxes\*. _J Clin Epidemiol_, 43(6), 543–549.
